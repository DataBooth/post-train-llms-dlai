{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc935f2b-a58b-4110-8918-96c868047b38",
   "metadata": {},
   "source": [
    "# L3: Supervised Fine-Tuning (SFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5ce6fc",
   "metadata": {},
   "source": [
    "Supervised fine-tuning (SFT) is a process where a pre-trained model is further trained on a labeled dataset to adapt it to specific tasks or domains. This process helps the model learn task-specific patterns and improves its performance on those tasks.\n",
    "\n",
    "In this notebook we demonstrate the process of SFT starting with a pre-trained \"base\" model. In this case we are using the [Qwen3-0.6B-Base](https://huggingface.co/Qwen/Qwen3-0.6B-Base) model, which is a 6 billion parameter model trained by Alibaba on a large corpus of text data.\n",
    "\n",
    "Qwen3-0.6B-Base is a highly capable, multilingual, and efficient open-source language model with a 32k context window, strong reasoning and code abilities, and a modern, robust training pipeline. It is suitable for a variety of research and production text generation tasks, especially where memory or compute resources are limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b742eb",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant User\n",
    "    participant Tokenizer\n",
    "    participant Model\n",
    "\n",
    "    %% User provides each question (prompt) to the tokenizer\n",
    "    User->>Tokenizer: \"Give me an 1-sentence introduction of LLM.\"\n",
    "    User->>Tokenizer: \"Calculate 1+1-1\"\n",
    "    User->>Tokenizer: \"What's the difference between thread and process?\"\n",
    "\n",
    "    %% Tokenizer processes each question\n",
    "    Tokenizer->>Tokenizer: tokenize & encode\n",
    "    Tokenizer-->>User: {\"input_ids\": [...], \"attention_mask\": [...]}\n",
    "\n",
    "    %% User sends encoded inputs to the model\n",
    "    User->>Model: model(**tokenized_question)\n",
    "    Model->>Model: forward pass\n",
    "    Model-->>User: output_ids\n",
    "\n",
    "    %% User decodes model output\n",
    "    User->>Tokenizer: decode(output_ids)\n",
    "    Tokenizer-->>User: output_text (e.g., answer)\n",
    "```\n",
    "---\n",
    "\n",
    "### **Step-by-step Example for the First Question**\n",
    "\n",
    "| Step                                 | Example Input/Output                                                                                       |\n",
    "|---------------------------------------|-----------------------------------------------------------------------------------------------------------|\n",
    "| **Input question**                    | `\"Give me an 1-sentence introduction of LLM.\"`                                                           |\n",
    "| **Tokenisation**                      | Tokens: `['Give', 'me', 'an', '1', '-', 'sentence', ...]`Token IDs: `[2001, 2033, 2019, 1015, ...]`   |\n",
    "| **Model input**                       | `input_ids`, `attention_mask`                                                                             |\n",
    "| **Model output**                      | `output_ids` (e.g., `[101, 2023, 2003, 1037, 6251, ...]`)                                                 |\n",
    "| **Decoded output**                    | `\"A large language model (LLM) is a type of AI that...\"`                                                  |\n",
    "\n",
    "### **Notes**\n",
    "- This workflow applies to each question in the input list.\n",
    "- For batch inference, the tokeniser and model can process all questions at once (as a batch), or you can loop as above.\n",
    "- The decoded output is the model's answer to each prompt.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea483fc9",
   "metadata": {},
   "source": [
    "### Warning controls - suppress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d76b86a7-7a45-4749-866e-0d561041bce1",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbfc986-9ac7-4a2d-9dd0-a76841c7f46d",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3304e49d-bd1e-469b-a5b4-5edb16ecf344",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import DataCollatorForCompletionOnlyLM, SFTConfig, SFTTrainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc63b02-5e9a-4a83-b042-4a2386cf5976",
   "metadata": {},
   "source": [
    "## Setting up helper class to manage Hugging Face API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5fed948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HuggingFaceManager:\n",
    "    \"\"\"\n",
    "    HuggingFaceManager provides a unified interface for managing Hugging Face models, tokenisers, and datasets.\n",
    "\n",
    "    What:\n",
    "        - Loads, caches, and lists Hugging Face models and datasets.\n",
    "        - Supports both online and offline workflows.\n",
    "        - Handles cache directory resolution and environment configuration.\n",
    "        - Offers configuration introspection and local resource discovery.\n",
    "        - Supports basic text generation using the loaded model and tokeniser.\n",
    "\n",
    "    Why:\n",
    "        - Simplifies reproducible, robust LLM workflows.\n",
    "        - Centralises Hugging Face infrastructure logic for maintainability and clarity.\n",
    "        - Facilitates offline/online toggling and local resource management.\n",
    "\n",
    "    How:\n",
    "        - Uses environment variables and standard Hugging Face conventions.\n",
    "        - Provides type hints, docstrings, and loguru logging for transparency and debugging.\n",
    "        - Designed for integration in notebooks or modular codebases.\n",
    "\n",
    "    ---------------------------------------------------------------------------\n",
    "    ENVIRONMENT VARIABLES (Purpose and Defaults):\n",
    "\n",
    "    - HF_HOME:\n",
    "        * Purpose: Sets the base directory for Hugging Face cache and configuration files.\n",
    "        * Default: ~/.cache/huggingface\n",
    "\n",
    "    - TRANSFORMERS_CACHE:\n",
    "        * Purpose: Sets the directory for caching Hugging Face Transformers models and tokenisers.\n",
    "        * Default: $HF_HOME/transformers\n",
    "\n",
    "    - HF_DATASETS_CACHE:\n",
    "        * Purpose: Sets the directory for caching Hugging Face datasets.\n",
    "        * Default: $HF_HOME/datasets\n",
    "\n",
    "    - HUGGINGFACE_HUB_CACHE:\n",
    "        * Purpose: Sets the directory for caching repositories from the Hub (models, datasets, spaces).\n",
    "        * Default: $HF_HOME/hub\n",
    "\n",
    "    - HF_HUB_OFFLINE:\n",
    "        * Purpose: If set (e.g. \"1\"), disables all network access and forces offline mode.\n",
    "        * Default: Not set (online mode)\n",
    "\n",
    "    - HUGGING_FACE_HUB_TOKEN or HF_TOKEN:\n",
    "        * Purpose: User access token for authenticating to the Hugging Face Hub.\n",
    "        * Default: Not set (anonymous access)\n",
    "\n",
    "    For further details, see: https://huggingface.co/docs/huggingface_hub/en/package_reference/environment_variables\n",
    "\n",
    "    ---------------------------------------------------------------------------\n",
    "    \n",
    "    DEFAULT SETTINGS FOR THIS CLASS:\n",
    "\n",
    "    - cache_dir: None (uses Hugging Face defaults)\n",
    "    - offline: False (online mode by default)\n",
    "    - use_gpu: False (CPU by default)\n",
    "    - verbose: True (logging enabled)\n",
    "    - model_name: None (no model loaded initially)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cache_dir: Optional[str] = None,\n",
    "        offline: bool = False,\n",
    "        use_gpu: bool = False,\n",
    "        verbose: bool = True,\n",
    "        model_name: Optional[str] = None, \n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialises the manager and configures environment variables.\n",
    "\n",
    "        Args:\n",
    "            cache_dir: Custom cache directory. If None, uses Hugging Face defaults.\n",
    "            offline: If True, enables offline mode (no network access).\n",
    "            use_gpu: If True, moves models to GPU.\n",
    "            verbose: If True, enables info-level logging.\n",
    "            model_name: If provided, loads this model/tokeniser immediately.\n",
    "        \"\"\"\n",
    "        self.cache_dir = cache_dir\n",
    "        self.offline = offline\n",
    "        self.use_gpu = use_gpu\n",
    "        self.verbose = verbose\n",
    "        self.model: Optional[PreTrainedModel] = None\n",
    "        self.tokenizer: Optional[PreTrainedTokenizer] = None\n",
    "        self.model_name: Optional[str] = None  \n",
    "        self._set_env_vars()\n",
    "        if model_name is not None:\n",
    "            self.load_model_and_tokenizer(model_name)\n",
    "\n",
    "    def _set_env_vars(self) -> None:\n",
    "        \"\"\"Set environment variables for tokenizer, cache and offline mode.\"\"\"\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"   # Always disable tokenizers parallelism to avoid fork warnings\n",
    "        if self.cache_dir:\n",
    "            os.environ[\"TRANSFORMERS_CACHE\"] = self.cache_dir\n",
    "            os.environ[\"HF_HOME\"] = self.cache_dir\n",
    "        if self.offline:\n",
    "            os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "        else:\n",
    "            os.environ.pop(\"HF_HUB_OFFLINE\", None)\n",
    "        if self.verbose:\n",
    "            logger.info(f\"Cache directory: {self.get_base_cache_dir()}\")\n",
    "            logger.info(f\"Offline mode: {self.offline}\")\n",
    "\n",
    "    def get_base_cache_dir(self) -> Path:\n",
    "        \"\"\"\n",
    "        Resolve the base Hugging Face cache directory.\n",
    "\n",
    "        Returns:\n",
    "            Path to the base cache directory.\n",
    "        \"\"\"\n",
    "        return Path(\n",
    "            self.cache_dir or\n",
    "            os.environ.get(\"TRANSFORMERS_CACHE\") or\n",
    "            os.environ.get(\"HF_HOME\") or\n",
    "            Path.home() / \".cache\" / \"huggingface\"\n",
    "        )\n",
    "\n",
    "    def get_hub_cache_dir(self) -> Path:\n",
    "        \"\"\"\n",
    "        Return the hub subdirectory for cached models and datasets.\n",
    "\n",
    "        Returns:\n",
    "            Path to the hub cache directory.\n",
    "        \"\"\"\n",
    "        return self.get_base_cache_dir() / \"hub\"\n",
    "    \n",
    "    def get_model_name(self) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Returns the currently loaded model name, or None if not loaded.\n",
    "        \"\"\"\n",
    "        return self.model_name\n",
    "\n",
    "    def display_configuration_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Print and return a summary of the Hugging Face environment configuration.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing configuration details.\n",
    "        \"\"\"\n",
    "        default_hf_home = str(Path.home() / \".cache\" / \"huggingface\")\n",
    "        default_transformers_cache = os.path.join(default_hf_home, \"transformers\")\n",
    "        default_datasets_cache = os.path.join(default_hf_home, \"datasets\")\n",
    "\n",
    "        transformers_cache = os.environ.get(\"TRANSFORMERS_CACHE\", default_transformers_cache)\n",
    "        hf_home = os.environ.get(\"HF_HOME\", default_hf_home)\n",
    "        hf_datasets_cache = os.environ.get(\"HF_DATASETS_CACHE\", default_datasets_cache)\n",
    "        tokenizers_parallelism = os.environ.get(\"TOKENIZERS_PARALLELISM\", None) \n",
    "\n",
    "        cache_dir_display = str(self.get_base_cache_dir())\n",
    "\n",
    "        model_id = self.model_name  \n",
    "        hub_url = self.get_hf_hub_url(model_id) if model_id else None\n",
    "    \n",
    "        summary = {\n",
    "            \"Cache Directory\": cache_dir_display,\n",
    "            \"TRANSFORMERS_CACHE\": transformers_cache,\n",
    "            \"HF_HOME\": hf_home,\n",
    "            \"HF_DATASETS_CACHE\": hf_datasets_cache,\n",
    "            \"Offline Mode\": self.offline,\n",
    "            \"Use GPU\": self.use_gpu,\n",
    "            \"Verbose\": self.verbose,\n",
    "            \"Hub URL\": hub_url,\n",
    "            \"TOKENIZERS_PARALLELISM\": tokenizers_parallelism, \n",
    "        }\n",
    "\n",
    "        logger.info(\"Hugging Face Manager Configuration Summary:\")\n",
    "        for k, v in summary.items():\n",
    "            logger.info(f\"{k}: {v}\")\n",
    "\n",
    "        return summary\n",
    "\n",
    "\n",
    "    def load_model_and_tokenizer(\n",
    "            self, model_name: str\n",
    "        ) -> None:\n",
    "            \"\"\"\n",
    "            Load a Hugging Face model and tokeniser, preferring cache/local files if offline.\n",
    "            Stores them as instance variables.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                logger.info(f\"Loading model and tokeniser: {model_name}\")\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    model_name,\n",
    "                    cache_dir=str(self.get_base_cache_dir()),\n",
    "                    local_files_only=self.offline\n",
    "                )\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    cache_dir=str(self.get_base_cache_dir()),\n",
    "                    local_files_only=self.offline\n",
    "                )\n",
    "                if self.use_gpu and self.model is not None:\n",
    "                    self.model.to(\"cuda\")\n",
    "                if self.tokenizer and not self.tokenizer.pad_token:\n",
    "                    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                self.model_name = model_name  \n",
    "                logger.info(f\"Loaded model and tokeniser for: {model_name}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to load model/tokeniser: {e}\")\n",
    "                raise\n",
    "\n",
    "    def load_dataset(\n",
    "        self, dataset_name: str, split: str = \"train\", **kwargs\n",
    "    ) -> Dataset:\n",
    "        \"\"\"\n",
    "        Load a Hugging Face dataset, preferring cache/local files if offline.\n",
    "\n",
    "        Args:\n",
    "            dataset_name: Name or path of the dataset.\n",
    "            split: Dataset split to load (e.g. \"train\").\n",
    "            **kwargs: Additional arguments for load_dataset.\n",
    "\n",
    "        Returns:\n",
    "            Loaded Dataset object.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading dataset: {dataset_name} (split={split})\")\n",
    "            try:\n",
    "                dataset = load_dataset(\n",
    "                    dataset_name,\n",
    "                    split=split,\n",
    "                    cache_dir=str(self.get_base_cache_dir()),\n",
    "                    local_files_only=self.offline,\n",
    "                    **kwargs\n",
    "                )\n",
    "            except (TypeError, ValueError) as e:\n",
    "                if \"local_files_only\" in str(e):\n",
    "                    logger.warning(\"Retrying without local_files_only (not supported by this dataset builder).\")\n",
    "                    dataset = load_dataset(\n",
    "                        dataset_name,\n",
    "                        split=split,\n",
    "                        cache_dir=str(self.get_base_cache_dir()),\n",
    "                        **kwargs\n",
    "                    )\n",
    "                else:\n",
    "                    raise\n",
    "            logger.info(f\"Loaded dataset: {dataset_name}\")\n",
    "            return dataset\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load dataset: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "    def check_model_downloaded(self, model_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a model is available locally in the cache.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name or path of the model.\n",
    "\n",
    "        Returns:\n",
    "            True if the model is cached locally, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            _ = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=str(self.get_base_cache_dir()),\n",
    "                local_files_only=True\n",
    "            )\n",
    "            logger.info(f\"Model '{model_name}' is available locally.\")\n",
    "            return True\n",
    "        except Exception:\n",
    "            logger.info(f\"Model '{model_name}' is NOT available locally.\")\n",
    "            return False\n",
    "\n",
    "    def display_dataset(self, dataset: Dataset, n: int = 3) -> None:\n",
    "        \"\"\"\n",
    "        Display a sample of the dataset in tabular form.\n",
    "\n",
    "        Args:\n",
    "            dataset: The Hugging Face Dataset object.\n",
    "            n: Number of rows to display.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        for i in range(min(n, len(dataset))):\n",
    "            example = dataset[i]\n",
    "            user_msg = next(m['content'] for m in example['messages'] if m['role'] == 'user')\n",
    "            assistant_msg = next(m['content'] for m in example['messages'] if m['role'] == 'assistant')\n",
    "            rows.append({'User Prompt': user_msg, 'Assistant Response': assistant_msg})\n",
    "        df = pd.DataFrame(rows)\n",
    "        pd.set_option('display.max_colwidth', None)\n",
    "        logger.info(f\"Displaying {n} rows from dataset.\")\n",
    "        display(df)\n",
    "\n",
    "    def list_local_models(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        List all locally cached Hugging Face models.\n",
    "\n",
    "        Returns:\n",
    "            List of model names available in the local cache.\n",
    "        \"\"\"\n",
    "        hub_dir = self.get_hub_cache_dir()\n",
    "        model_dirs = sorted(hub_dir.glob(\"models--*\"))\n",
    "        models = [d.name.replace(\"models--\", \"\") for d in model_dirs if d.is_dir()]\n",
    "        logger.info(\"Locally cached models:\")\n",
    "        for m in models:\n",
    "            logger.info(f\"- {m}\")\n",
    "        return models\n",
    "\n",
    "    def list_local_datasets(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        List all locally cached Hugging Face datasets.\n",
    "\n",
    "        Returns:\n",
    "            List of dataset names available in the local cache.\n",
    "        \"\"\"\n",
    "        hub_dir = self.get_hub_cache_dir()\n",
    "        dataset_dirs = sorted(hub_dir.glob(\"datasets--*\"))\n",
    "        datasets = [d.name.replace(\"datasets--\", \"\") for d in dataset_dirs if d.is_dir()]\n",
    "        logger.info(\"Locally cached datasets:\")\n",
    "        for d in datasets:\n",
    "            logger.info(f\"- {d}\")\n",
    "        return datasets\n",
    "\n",
    "    def list_local_transformers(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Alias for listing locally cached models.\n",
    "\n",
    "        Returns:\n",
    "            List of model names available in the local cache.\n",
    "        \"\"\"\n",
    "        return self.list_local_models()\n",
    "\n",
    "    def generate_response(\n",
    "        self,\n",
    "        user_message: str,\n",
    "        system_message: Optional[str] = None,\n",
    "        max_new_tokens: int = 100\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response using the loaded model and tokeniser.\n",
    "\n",
    "        Args:\n",
    "            user_message: The user's message string.\n",
    "            system_message: Optional system message string.\n",
    "            max_new_tokens: Maximum number of new tokens to generate.\n",
    "\n",
    "        Returns:\n",
    "            The generated response as a string.\n",
    "        \"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise ValueError(\"Model and tokeniser must be loaded first.\")\n",
    "\n",
    "        messages = []\n",
    "        if system_message:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=False,\n",
    "        )\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "        input_len = inputs[\"input_ids\"].shape[1]\n",
    "        generated_ids = outputs[0][input_len:]\n",
    "        response = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "        logger.info(\"Generated response.\")\n",
    "        return response\n",
    "\n",
    "    def unload_model_and_tokenizer(self) -> None:\n",
    "        \"\"\"\n",
    "        Remove references to the loaded model and tokeniser to free memory.\n",
    "        \"\"\"\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.model_name = None   # <-- UNSET MODEL NAME HERE\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_hf_hub_url(model_name: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Convert a Hugging Face model or dataset identifier to a Hub URL.\n",
    "\n",
    "        Args:\n",
    "            model_name: The model or dataset identifier, e.g., \"Qwen/Qwen3-0.6B-Base\".\n",
    "\n",
    "        Returns:\n",
    "            The corresponding Hugging Face Hub URL, or None if the format is invalid.\n",
    "        \"\"\"\n",
    "        if not isinstance(model_name, str) or \"/\" not in model_name:\n",
    "            return None\n",
    "        return f\"https://huggingface.co/{model_name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "234e5b05-a493-4683-91fd-7417885efc0f",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "def test_model_with_questions(\n",
    "    manager: HuggingFaceManager,\n",
    "    questions: list,\n",
    "    system_message: Optional[str] = None,\n",
    "    title: str = \"Model Output\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Test the loaded model in the HuggingFaceManager with a list of questions.\n",
    "    \"\"\"\n",
    "    if manager.model is None or manager.tokenizer is None:\n",
    "        raise ValueError(\"Model and tokeniser must be loaded first.\")\n",
    "\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        response = manager.generate_response(\n",
    "            user_message=question,\n",
    "            system_message=system_message\n",
    "        )\n",
    "        print(f\"\\nModel Input {i}:\\n{question}\\nModel Output {i}:\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c8ac19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-14 11:46:51.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_set_env_vars\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mCache directory: /Users/mjboothaus/.cache/huggingface\u001b[0m\n",
      "\u001b[32m2025-07-14 11:46:51.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_set_env_vars\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mOffline mode: False\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = False\n",
    "\n",
    "hf = HuggingFaceManager(use_gpu=USE_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ac817-43a4-43c9-88f3-9825b96b84b7",
   "metadata": {},
   "source": [
    "## Load base model & test on simple questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba83dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-14 11:46:51.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model_and_tokenizer\u001b[0m:\u001b[36m182\u001b[0m - \u001b[1mLoading model and tokeniser: Qwen/Qwen3-0.6B-Base\u001b[0m\n",
      "\u001b[32m2025-07-14 11:46:55.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model_and_tokenizer\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mLoaded model and tokeniser for: Qwen/Qwen3-0.6B-Base\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL = \"Qwen/Qwen3-0.6B-Base\"\n",
    "\n",
    "hf.load_model_and_tokenizer(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99cea8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-14 11:46:55.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdisplay_configuration_summary\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mHugging Face Manager Configuration Summary:\u001b[0m\n",
      "\u001b[32m2025-07-14 11:46:55.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdisplay_configuration_summary\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mCache Directory: /Users/mjboothaus/.cache/huggingface\u001b[0m\n",
      "\u001b[32m2025-07-14 11:46:55.956\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdisplay_configuration_summary\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mTRANSFORMERS_CACHE: /Users/mjboothaus/.cache/huggingface/transformers\u001b[0m\n",
      "\u001b[32m2025-07-14 11:46:55.956\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdisplay_configuration_summary\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mHF_HOME: /Users/mjboothaus/.cache/huggingface\u001b[0m\n",
      "\u001b[32m2025-07-14 11:46:55.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdisplay_configuration_summary\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mHF_DATASETS_CACHE: /Users/mjboothaus/.cache/huggingface/datasets\u001b[0m\n",
      "\u001b[32m2025-07-14 11:46:55.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdisplay_configuration_summary\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mOffline Mode: False\u001b[0m\n",
      "\u001b[32m2025-07-14 11:46:55.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdisplay_configuration_summary\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mUse GPU: False\u001b[0m\n",
      "\u001b[32m2025-07-14 11:46:55.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdisplay_configuration_summary\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mVerbose: True\u001b[0m\n",
      "\u001b[32m2025-07-14 11:46:55.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdisplay_configuration_summary\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mHub URL: https://huggingface.co/Qwen/Qwen3-0.6B-Base\u001b[0m\n",
      "\u001b[32m2025-07-14 11:46:55.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdisplay_configuration_summary\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mTOKENIZERS_PARALLELISM: false\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Cache Directory': '/Users/mjboothaus/.cache/huggingface',\n",
       " 'TRANSFORMERS_CACHE': '/Users/mjboothaus/.cache/huggingface/transformers',\n",
       " 'HF_HOME': '/Users/mjboothaus/.cache/huggingface',\n",
       " 'HF_DATASETS_CACHE': '/Users/mjboothaus/.cache/huggingface/datasets',\n",
       " 'Offline Mode': False,\n",
       " 'Use GPU': False,\n",
       " 'Verbose': True,\n",
       " 'Hub URL': 'https://huggingface.co/Qwen/Qwen3-0.6B-Base',\n",
       " 'TOKENIZERS_PARALLELISM': 'false'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf.display_configuration_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fed78c2-ea93-4ac2-bd6f-5d4391de7c8d",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Give me an 1-sentence introduction of LLM.\",\n",
    "    \"Calculate 1+1-1\",\n",
    "    \"What's the difference between thread and process?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8696d1",
   "metadata": {},
   "source": [
    "## Base Model (Before SFT) Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba426c74-4d93-42b3-b2c7-5791fb9bf3c5",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base Model (Before SFT) Output ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-14 11:47:08.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m357\u001b[0m - \u001b[1mGenerated response.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ �\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-14 11:47:20.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m357\u001b[0m - \u001b[1mGenerated response.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Input 2:\n",
      "Calculate 1+1-1\n",
      "Model Output 2:\n",
      "⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ �\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-14 11:47:33.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m357\u001b[0m - \u001b[1mGenerated response.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Input 3:\n",
      "What's the difference between thread and process?\n",
      "Model Output 3:\n",
      "⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ �\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model_with_questions(hf, questions, title=\"Base Model (Before SFT) Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3235833",
   "metadata": {},
   "source": [
    "*Note: In the outputs above, the model output is not as expected, indicating that the model has not been fine-tuned yet.*\n",
    "\n",
    "The tokenizer is able to tokenize the input questions, but the model's responses are not aligned with the expected answers. This is expected behavior since we are using a base model that has not undergone any fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14ad5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.unload_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885253e0-3a9a-4b42-a36b-a8a3ddd340a1",
   "metadata": {},
   "source": [
    "## SFT results on Qwen3-0.6B model\n",
    "\n",
    "In this section, we're reviewing the results of a previously completed SFT training. Due to limited resources, we won’t be running the full training on a relatively large model like Qwen3-0.6B. However, in the next section of this notebook, you’ll walk through the full training process using a smaller model and a lightweight dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7921361",
   "metadata": {},
   "outputs": [],
   "source": [
    "QWEN_SMALL_SFT = \"banghua/Qwen3-0.6B-SFT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e176c328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-14 11:47:33.765\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model_and_tokenizer\u001b[0m:\u001b[36m182\u001b[0m - \u001b[1mLoading model and tokeniser: banghua/Qwen3-0.6B-SFT\u001b[0m\n",
      "\u001b[32m2025-07-14 11:47:35.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model_and_tokenizer\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mLoaded model and tokeniser for: banghua/Qwen3-0.6B-SFT\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "hf.load_model_and_tokenizer(QWEN_SMALL_SFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e86f13c-c969-4c7e-8702-d074ee7a2ce6",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base Model (After SFT) Output ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-14 11:47:41.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m357\u001b[0m - \u001b[1mGenerated response.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "LLM is a program that provides advanced legal knowledge and skills to professionals and individuals.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-14 11:47:45.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m357\u001b[0m - \u001b[1mGenerated response.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Input 2:\n",
      "Calculate 1+1-1\n",
      "Model Output 2:\n",
      "1+1-1 = 2-1 = 1\n",
      "\n",
      "So, the final answer is 1.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-14 11:47:57.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m357\u001b[0m - \u001b[1mGenerated response.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Input 3:\n",
      "What's the difference between thread and process?\n",
      "Model Output 3:\n",
      "In computer science, a thread is a unit of execution that runs in a separate process. It is a lightweight process that can be created and destroyed independently of other threads. Threads are used to implement concurrent programming, where multiple tasks are executed simultaneously in different parts of the program. Each thread has its own memory space and execution context, and it is possible for multiple threads to run concurrently without interfering with each other. Threads are also known as lightweight processes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model_with_questions(hf, questions, title=\"Base Model (After SFT) Output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "997ac289",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.unload_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf299ee-aa84-4c43-8d7b-f0998077e2cb",
   "metadata": {},
   "source": [
    "## Doing SFT on a small model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5cb7ea-e157-418e-84f5-34ecbed823ad",
   "metadata": {},
   "source": [
    "**Note:** We're performing SFT on a small model <code>HuggingFaceTB/SmolLM2-135M</code> and a smaller training dataset to to ensure the full training process can run on limited computational resources. If you're running the notebooks on your own machine and have access to a GPU, feel free to switch to a larger model—such as <code>Qwen/Qwen3-0.6B-Base</code>—to perform full SFT and reproduce the results shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdb07589-049d-432e-8001-e6e9175ad806",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-14 11:50:41.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model_and_tokenizer\u001b[0m:\u001b[36m182\u001b[0m - \u001b[1mLoading model and tokeniser: HuggingFaceTB/SmolLM2-135M\u001b[0m\n",
      "\u001b[32m2025-07-14 11:50:43.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model_and_tokenizer\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mLoaded model and tokeniser for: HuggingFaceTB/SmolLM2-135M\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "hf.load_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff149c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 2961\n",
      "})\n",
      "{'messages': [{'content': \"- The left child should have a value less than the parent node's value, and the right child should have a value greater than the parent node's value.\", 'role': 'user'}, {'content': \"This statement is correct. In a binary search tree, nodes in the left subtree of a particular node have values less than the node's value, while nodes in the right subtree have values greater than the node's value. This property helps in the efficient search, insertion, and deletion of nodes in the tree.\", 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"banghua/DL-SFT-Dataset\", split=\"train\")\n",
    "print(train_dataset)\n",
    "print(train_dataset[0])  # Show the first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a17ed4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': \"- The left child should have a value less than the parent node's \"\n",
      "             'value, and the right child should have a value greater than the '\n",
      "             \"parent node's value.\",\n",
      "  'role': 'user'},\n",
      " {'content': 'This statement is correct. In a binary search tree, nodes in the '\n",
      "             'left subtree of a particular node have values less than the '\n",
      "             \"node's value, while nodes in the right subtree have values \"\n",
      "             \"greater than the node's value. This property helps in the \"\n",
      "             'efficient search, insertion, and deletion of nodes in the tree.',\n",
      "  'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(train_dataset[0]['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "488c3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf.load_dataset(\"banghua/DL-SFT-Dataset\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d52c3e-9c6c-43c3-bd95-92d60b9c9a8f",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "# train_dataset = load_dataset(\"banghua/DL-SFT-Dataset\")[\"train\"]\n",
    "# if not USE_GPU:\n",
    "#     train_dataset=train_dataset.select(range(100))\n",
    "\n",
    "# display_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35bb838",
   "metadata": {},
   "source": [
    "### `SFTTrainer` config \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01c515a8-3728-45fa-88cc-6eb4de839839",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    learning_rate=8e-5, # Learning rate for training. \n",
    "    num_train_epochs=1, #  Set the number of epochs to train the model.\n",
    "    per_device_train_batch_size=1, # Batch size for each device (e.g., GPU) during training. \n",
    "    gradient_accumulation_steps=8, # Number of steps before performing a backward/update pass to accumulate gradients.\n",
    "    gradient_checkpointing=False, # Enable gradient checkpointing to reduce memory usage during training at the cost of slower training speed.\n",
    "    logging_steps=2,  # Frequency of logging training progress (log every 2 steps).\n",
    "    bf16=False,      # Disable bfloat16 (required for CPU or unsupported GPU)\n",
    "    fp16=False,      # Disable float16 (required for CPU or unsupported GPU)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cd7d132-4c69-4b12-a8ec-e6b4795faad9",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset:   0%|          | 0/2961 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m sft_trainer = \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/github/databooth/post-train-llms-dlai/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:481\u001b[39m, in \u001b[36mSFTTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.completion_only_loss \u001b[38;5;129;01mand\u001b[39;00m formatting_func:\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    476\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA formatting function was provided while `completion_only_loss=True`, which is incompatible. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    477\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing a formatter converts the dataset to a language modeling type, conflicting with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    478\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcompletion-only loss. To resolve this, apply your formatting function before passing the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    479\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdataset, or disable `completion_only_loss` in `SFTConfig`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    480\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m train_dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpacking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    483\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    485\u001b[39m     packing = args.packing \u001b[38;5;28;01mif\u001b[39;00m args.eval_packing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m args.eval_packing\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/github/databooth/post-train-llms-dlai/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:778\u001b[39m, in \u001b[36mSFTTrainer._prepare_dataset\u001b[39m\u001b[34m(self, dataset, processing_class, args, packing, formatting_func, dataset_name)\u001b[39m\n\u001b[32m    775\u001b[39m                 processed = {\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: processing_class(text=example[dataset_text_field]).input_ids}\n\u001b[32m    776\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m processed\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     dataset = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprocessing_class\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset_text_field\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43massistant_only_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43massistant_only_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmap_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[38;5;66;03m# Pack or truncate\u001b[39;00m\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m packing:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/github/databooth/post-train-llms-dlai/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:560\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    553\u001b[39m self_format = {\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    555\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    558\u001b[39m }\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/github/databooth/post-train-llms-dlai/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3318\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3316\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3317\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m unprocessed_kwargs \u001b[38;5;129;01min\u001b[39;00m unprocessed_kwargs_per_job:\n\u001b[32m-> \u001b[39m\u001b[32m3318\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munprocessed_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3319\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3321\u001b[39m \u001b[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/github/databooth/post-train-llms-dlai/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3650\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3648\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[32m   3649\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3650\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3651\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3652\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/github/databooth/post-train-llms-dlai/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3624\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3622\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3623\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3624\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/github/databooth/post-train-llms-dlai/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3547\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3545\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3546\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3547\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3548\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/github/databooth/post-train-llms-dlai/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:758\u001b[39m, in \u001b[36mSFTTrainer._prepare_dataset.<locals>.tokenize\u001b[39m\u001b[34m(example, processing_class, dataset_text_field, assistant_only_loss)\u001b[39m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# language modeling case\u001b[39;00m\n\u001b[32m    757\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_conversational(example):\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m         processed = \u001b[43mprocessing_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43massistant_only_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchat_template_kwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    765\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33massistant_masks\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m processed \u001b[38;5;129;01mand\u001b[39;00m \u001b[32m1\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed[\u001b[33m\"\u001b[39m\u001b[33massistant_masks\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    766\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    767\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mYou\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre using `assistant_only_loss=True`, but at least one example has no \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    768\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33massistant tokens. This usually means the tokenizer\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms chat template doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    771\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmasking.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    772\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/github/databooth/post-train-llms-dlai/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1621\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.apply_chat_template\u001b[39m\u001b[34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[39m\n\u001b[32m   1618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1619\u001b[39m     tokenizer_kwargs = {}\n\u001b[32m-> \u001b[39m\u001b[32m1621\u001b[39m chat_template = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(conversation, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m   1624\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(conversation[\u001b[32m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conversation[\u001b[32m0\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1625\u001b[39m ):\n\u001b[32m   1626\u001b[39m     conversations = conversation\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/github/databooth/post-train-llms-dlai/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1743\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.get_chat_template\u001b[39m\u001b[34m(self, chat_template, tools)\u001b[39m\n\u001b[32m   1741\u001b[39m         chat_template = \u001b[38;5;28mself\u001b[39m.chat_template\n\u001b[32m   1742\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1744\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1745\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33margument was passed! For information about writing templates and setting the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1746\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1747\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1748\u001b[39m         )\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[31mValueError\u001b[39m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "sft_trainer = SFTTrainer(\n",
    "    model=hf.model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_dataset, \n",
    "    processing_class=hf.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b4bac1-7262-4c55-b411-6a59188157b0",
   "metadata": {},
   "source": [
    "## Testing training results on small model and small dataset\n",
    "\n",
    "**Note:** The following results are for the small model and dataset we used for SFT training, due to limited computational resources. To view the results of full-scale training on a larger model, see the **\"SFT Results on Qwen3-0.6B Model\"** section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d678274-5768-4cea-ae20-051488e5d0f3",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "if not USE_GPU: # move model to CPU when GPU isn’t requested\n",
    "    sft_trainer.model.to(\"cpu\")\n",
    "test_model_with_questions(sft_trainer.model, tokenizer, questions, \n",
    "                          title=\"Base Model (After SFT) Output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d5896-6fd6-43d2-85f1-dacbd594f4cf",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3c0bba-984a-494c-8374-33db30ad1da6",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "post-train-llms-dlai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
